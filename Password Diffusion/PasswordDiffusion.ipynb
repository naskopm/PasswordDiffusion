{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bd71443-7f9f-42a0-b9e7-ed49c5cea03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_20320\\2018655843.py:211: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_20320\\2018655843.py:228: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_20320\\2018655843.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 1/5 completed. Train Loss: 3.5438\n",
      "=> Epoch 2/5 completed. Train Loss: 3.4943\n",
      "=> Epoch 3/5 completed. Train Loss: 3.4922\n",
      "=> Epoch 4/5 completed. Train Loss: 3.4914\n",
      "=> Epoch 5/5 completed. Train Loss: 3.4909\n",
      "[DEBUG] Saved new model to 5MIL_Reguralization.pth\n",
      "[DEBUG] Generated Samples:\n",
      "Sample #1: paoeaea1\n",
      "Sample #2: baira0\n",
      "Sample #3: mimsa212\n",
      "Sample #4: tanlne\n",
      "Sample #5: stnsler1\n",
      "Sample #6: brra1eon\n",
      "Sample #7: tonlisoa121\n",
      "Sample #8: vamota23111\n",
      "Sample #9: paan1229\n",
      "Sample #10: benaoo\n",
      "Sample #11: riakae10an0eo¸\n",
      "Sample #12: tannai2e\n",
      "Sample #13: saslera21\n",
      "Sample #14: siorioa\n",
      "Sample #15: sirots\n",
      "Sample #16: seaetn\n",
      "Sample #17: bolninao3o\n",
      "Sample #18: suneiio01oa\n",
      "Sample #19: riaaeree21a\n",
      "Sample #20: ponirenae\n",
      "Sample #21: suosneea\n",
      "Sample #22: taellni\n",
      "Sample #23: soallsee\n",
      "Sample #24: tiare21a\n",
      "Sample #25: punl1e2n\n",
      "Sample #26: yiilai1n0\n",
      "Sample #27: poaoler3\n",
      "Sample #28: sitolii0n\n",
      "Sample #29: mimee20221\n",
      "Sample #30: saot1erea\n",
      "Sample #31: yanairna\n",
      "Sample #32: toneya2a2\n",
      "Sample #33: tiae1e2en\n",
      "Sample #34: sarseaao\n",
      "Sample #35: soetnn1a21\n",
      "Sample #36: tals1o\n",
      "Sample #37: semerne\n",
      "Sample #38: ralalinae9oràs\n",
      "Sample #39: sateaa0a1a\n",
      "Sample #40: tonl11i\n",
      "Sample #41: puliaeao2ae\n",
      "Sample #42: pianyo1\n",
      "Sample #43: wartye11\n",
      "Sample #44: sasaaoaae1\n",
      "Sample #45: remel0n3\n",
      "Sample #46: tulko11\n",
      "Sample #47: pooarea1a\n",
      "Sample #48: seneen2912o\n",
      "Sample #49: weaeea\n",
      "Sample #50: waeia1en1e\n",
      "Sample #51: tomiia2\n",
      "Sample #52: peeoyeaa10\n",
      "Sample #53: seisaaa02\n",
      "Sample #54: tnree1r1\n",
      "Sample #55: bhenynnn\n",
      "Sample #56: ounaa2ra3\n",
      "Sample #57: yaeeni11\n",
      "Sample #58: saaaeon21\n",
      "Sample #59: seaaeai202ea\n",
      "Sample #60: raien1\n",
      "Sample #61: pillo0eo\n",
      "Sample #62: rloeyaia9\n",
      "Sample #63: potias\n",
      "Sample #64: womniii\n",
      "Sample #65: sinooiia\n",
      "Sample #66: saakranee\n",
      "Sample #67: sislii2111\n",
      "Sample #68: taraeea2\n",
      "Sample #69: rumit02211\n",
      "Sample #70: pelk1iaona0o\n",
      "Sample #71: pansin\n",
      "Sample #72: sanliae319\n",
      "Sample #73: soneren1a\n",
      "Sample #74: shml1022a\n",
      "Sample #75: sraeooeo19aa0s\n",
      "Sample #76: yorate0\n",
      "Sample #77: siaeno0a\n",
      "Sample #78: saloiore10\n",
      "Sample #79: posrne1\n",
      "Sample #80: roieroe1\n",
      "Sample #81: siloaa09\n",
      "Sample #82: sunstsn\n",
      "Sample #83: titeer0\n",
      "Sample #84: siaeaa11\n",
      "Sample #85: mellraa2aa\n",
      "Sample #86: saaee0ao\n",
      "Sample #87: sosreoe11era\n",
      "Sample #88: seolyaie0\n",
      "Sample #89: piear0en\n",
      "Sample #90: peln1a2\n",
      "Sample #91: seekyi1\n",
      "Sample #92: minii0rn\n",
      "Sample #93: wanier2\n",
      "Sample #94: yelnaan\n",
      "Sample #95: sookla120\n",
      "Sample #96: suiiaa\n",
      "Sample #97: veanaa11eee\n",
      "Sample #98: panaas11\n",
      "Sample #99: wirrlao\n",
      "Sample #100: raaenar0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "\n",
    "###############################################################################\n",
    "# 1) FORCE CUDA USAGE OR EXIT\n",
    "###############################################################################\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"ERROR: CUDA device not available! Exiting...\", flush=True)\n",
    "    sys.exit(1)\n",
    "DEVICE = \"cuda\"\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "###############################################################################\n",
    "# 2) HYPERPARAMETERS & SETTINGS\n",
    "###############################################################################\n",
    "MAX_SEQ_LEN = 32\n",
    "VOCAB_SIZE = 300\n",
    "EOS_TOKEN_ID = VOCAB_SIZE - 1   # Stop token\n",
    "MASK_TOKEN_ID = VOCAB_SIZE - 2  # Mask token\n",
    "PAD_TOKEN_ID = 0\n",
    "\n",
    "EMBED_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 8\n",
    "NUM_HEADS = 8\n",
    "\n",
    "BATCH_SIZE = 400       # Adjust according to your GPU memory\n",
    "LEARNING_RATE = 1e-4 \n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# L2 regularization (weight decay)\n",
    "L2_WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# Dropout rate for input and transformer layers.\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "# PENALTY FACTORS\n",
    "DIGIT_PENALTY = 1.0\n",
    "SPECIAL_PENALTY = 4.0\n",
    "EOS_WEIGHT = 0.3\n",
    "\n",
    "# KL divergence weight for regularizing the learned noise schedule.\n",
    "KL_WEIGHT = 0.1\n",
    "\n",
    "# Number of diffusion timesteps\n",
    "TIMESTEPS = 10\n",
    "\n",
    "# New: Optimized tau hyperparameter\n",
    "TAU = 0.05\n",
    "\n",
    "# Fixed cosine noise schedule (used to initialize the learnable noise schedule)\n",
    "def fixed_noise_schedule(timesteps, min_noise=0.02, max_noise=0.98):\n",
    "    return [min_noise + (max_noise - min_noise) * (1 - math.cos(math.pi * (i + 0.5) / timesteps)) / 2\n",
    "            for i in range(timesteps)]\n",
    "init_schedule = fixed_noise_schedule(TIMESTEPS, min_noise=0.02, max_noise=0.98)\n",
    "init_schedule_tensor = torch.tensor(init_schedule, dtype=torch.float32)\n",
    "\n",
    "PIN_MEMORY = True\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "###############################################################################\n",
    "# 3) DATASET CLASS\n",
    "###############################################################################\n",
    "class PasswordDataset(Dataset):\n",
    "    def __init__(self, file_path, max_seq_len=MAX_SEQ_LEN):\n",
    "        self.passwords = []\n",
    "        with open(file_path, \"r\", encoding=\"latin-1\", errors=\"ignore\") as f:\n",
    "            for line in f:\n",
    "                pwd = line.strip()\n",
    "                if pwd:\n",
    "                    self.passwords.append(pwd)\n",
    "        random.shuffle(self.passwords)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.passwords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pwd = self.passwords[idx]\n",
    "        token_ids = []\n",
    "        for c in pwd[:self.max_seq_len - 1]:\n",
    "            token = ord(c) - 31\n",
    "            token = max(1, min(token, VOCAB_SIZE - 3))\n",
    "            token_ids.append(token)\n",
    "        token_ids.append(EOS_TOKEN_ID)\n",
    "        token_ids += [PAD_TOKEN_ID] * (self.max_seq_len - len(token_ids))\n",
    "        return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "###############################################################################\n",
    "# 4) STOCHASTIC FORWARD DIFFUSION FUNCTION (with learnable noise)\n",
    "###############################################################################\n",
    "def forward_diffusion_continuous(x, t, noise_schedule, model, tau=TAU):\n",
    "    x0_emb = model.token_emb(x)  # shape: (B, T, D)\n",
    "    B, T, D = x0_emb.shape\n",
    "    mask_token_tensor = torch.tensor([MASK_TOKEN_ID], device=x.device)\n",
    "    mask_emb = model.token_emb(mask_token_tensor)  # shape: (1, D)\n",
    "    r = torch.rand(B, T, device=x.device)\n",
    "    if t.dim() == 0:\n",
    "        nf = noise_schedule[int(t.item())].item()\n",
    "        noise_fraction = torch.full((B, T), nf, device=x.device)\n",
    "    else:\n",
    "        noise_fraction = noise_schedule[t].unsqueeze(1).expand(B, T)\n",
    "    m = torch.sigmoid((r - noise_fraction) / tau)  # (B, T)\n",
    "    m = m.unsqueeze(-1)  # (B, T, 1)\n",
    "    return (1 - m) * x0_emb + m * mask_emb\n",
    "\n",
    "###############################################################################\n",
    "# 5) DIFFUSION TRANSFORMER MODEL (WITHOUT LENGTH PREDICTION HEAD)\n",
    "###############################################################################\n",
    "class DiffusionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embed_dim,\n",
    "                 hidden_dim,\n",
    "                 num_layers,\n",
    "                 num_heads,\n",
    "                 max_seq_len):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, embed_dim)\n",
    "        self.time_emb = nn.Embedding(TIMESTEPS, embed_dim)\n",
    "\n",
    "        self.input_norm = nn.LayerNorm(embed_dim)\n",
    "        self.input_dropout = nn.Dropout(DROPOUT_RATE)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=DROPOUT_RATE,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "        self.learned_noise_schedule = nn.Parameter(init_schedule_tensor.clone())\n",
    "\n",
    "    def forward(self, x, t, pre_embedded=False):\n",
    "        if not pre_embedded:\n",
    "            tokens_emb = self.token_emb(x)\n",
    "        else:\n",
    "            tokens_emb = x\n",
    "\n",
    "        positions = torch.arange(self.max_seq_len, device=x.device).unsqueeze(0)\n",
    "        pos_emb = self.pos_emb(positions)\n",
    "\n",
    "        if t.dim() == 0:\n",
    "            t_embed = self.time_emb(t).unsqueeze(0).unsqueeze(1).expand(x.size(0), self.max_seq_len, -1)\n",
    "        else:\n",
    "            t_embed = self.time_emb(t).unsqueeze(1).expand(-1, self.max_seq_len, -1)\n",
    "\n",
    "        x_input = tokens_emb + pos_emb + t_embed\n",
    "        x_input = self.input_norm(x_input)\n",
    "        x_input = self.input_dropout(x_input)\n",
    "\n",
    "        encoded = self.transformer_encoder(x_input)\n",
    "        logits = self.fc_out(encoded)\n",
    "        return logits\n",
    "\n",
    "###############################################################################\n",
    "# 6) REVERSE DIFFUSION UPDATE FUNCTION (with stochastic noise)\n",
    "###############################################################################\n",
    "def reverse_diffusion_update(predicted_x0_emb, t, model, base_noise_scale=0.05):\n",
    "    if t == 0:\n",
    "        return predicted_x0_emb\n",
    "    noise_fraction = model.learned_noise_schedule[t - 1]\n",
    "    mask_token_tensor = torch.tensor([MASK_TOKEN_ID], device=predicted_x0_emb.device)\n",
    "    mask_emb = model.token_emb(mask_token_tensor)\n",
    "    mask_emb = mask_emb.unsqueeze(0).expand_as(predicted_x0_emb)\n",
    "    noise_scale = base_noise_scale * (t / TIMESTEPS)\n",
    "    noise = noise_scale * torch.randn_like(predicted_x0_emb)\n",
    "    x_prev = (1 - noise_fraction) * predicted_x0_emb + noise_fraction * mask_emb + noise\n",
    "    return x_prev\n",
    "\n",
    "###############################################################################\n",
    "# 7) TRAINING FUNCTION (LOSS COMPUTED ONLY AT t=0, with KL divergence and\n",
    "#    repetition penalties for repeating characters and explicit EOS penalty)\n",
    "###############################################################################\n",
    "def train_model(model, train_loader, num_epochs):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_WEIGHT_DECAY)\n",
    "    total_steps = num_epochs * len(train_loader)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=LEARNING_RATE * 2,\n",
    "        total_steps=total_steps,\n",
    "        pct_start=0.3,\n",
    "        anneal_strategy='linear'\n",
    "    )\n",
    "    \n",
    "    token_loss_weights = torch.ones(VOCAB_SIZE)\n",
    "    for token in range(1, VOCAB_SIZE - 2):\n",
    "        ascii_val = token + 31\n",
    "        if 48 <= ascii_val <= 57:\n",
    "            token_loss_weights[token] = DIGIT_PENALTY\n",
    "        elif not (65 <= ascii_val <= 90 or 97 <= ascii_val <= 122):\n",
    "            token_loss_weights[token] = SPECIAL_PENALTY\n",
    "    token_loss_weights[EOS_TOKEN_ID] = EOS_WEIGHT\n",
    "\n",
    "    token_criterion = nn.CrossEntropyLoss(weight=token_loss_weights.to(DEVICE), ignore_index=PAD_TOKEN_ID)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    target_schedule = torch.softmax(init_schedule_tensor.to(DEVICE), dim=0)\n",
    "    \n",
    "    REPEAT_WEIGHT = 0.4\n",
    "    NUM_REPEAT_WEIGHT = 1.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0.0\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            x0 = batch.to(DEVICE)\n",
    "            batch_size = x0.size(0)\n",
    "            t_max = TIMESTEPS - 1\n",
    "            x = forward_diffusion_continuous(x0, torch.tensor(t_max, device=DEVICE),\n",
    "                                             model.learned_noise_schedule, model, tau=TAU)\n",
    "            for t in reversed(range(1, TIMESTEPS)):\n",
    "                t_tensor = torch.full((batch_size,), t, device=DEVICE, dtype=torch.long)\n",
    "                with torch.cuda.amp.autocast(enabled=True):\n",
    "                    logits = model(x, t_tensor, pre_embedded=True)\n",
    "                    probs = torch.softmax(logits, dim=-1)\n",
    "                    token_emb_weights = model.token_emb.weight\n",
    "                    predicted_x0_emb = torch.matmul(probs, token_emb_weights)\n",
    "                    x = reverse_diffusion_update(predicted_x0_emb, t, model)\n",
    "            t_tensor = torch.full((batch_size,), 0, device=DEVICE, dtype=torch.long)\n",
    "            with torch.cuda.amp.autocast(enabled=True):\n",
    "                final_logits = model(x, t_tensor, pre_embedded=True)\n",
    "                token_loss = token_criterion(final_logits.reshape(-1, model.vocab_size), x0.reshape(-1))\n",
    "                loss = token_loss\n",
    "\n",
    "                p = torch.softmax(model.learned_noise_schedule, dim=0)\n",
    "                kl_loss = torch.sum(p * (torch.log(p + 1e-8) - torch.log(target_schedule + 1e-8)))\n",
    "                \n",
    "                final_probs = torch.softmax(final_logits, dim=-1)\n",
    "                rep_penalty = 0.0\n",
    "                for i in range(1, MAX_SEQ_LEN):\n",
    "                    rep_penalty += torch.sum(final_probs[:, i] * final_probs[:, i-1])\n",
    "                rep_penalty = rep_penalty / (batch_size * (MAX_SEQ_LEN - 1))\n",
    "                \n",
    "                digit_indices = torch.arange(17, 27, device=DEVICE)\n",
    "                num_rep_penalty = 0.0\n",
    "                tail_range = range(MAX_SEQ_LEN - 3, MAX_SEQ_LEN) if MAX_SEQ_LEN >= 3 else range(1, MAX_SEQ_LEN)\n",
    "                for i in tail_range:\n",
    "                    p_i = final_probs[:, i][:, digit_indices]\n",
    "                    p_im1 = final_probs[:, i-1][:, digit_indices]\n",
    "                    num_rep_penalty += torch.sum(p_i * p_im1)\n",
    "                num_rep_penalty = num_rep_penalty / (batch_size * len(tail_range))\n",
    "                \n",
    "                total_loss = loss + KL_WEIGHT * kl_loss + REPEAT_WEIGHT * rep_penalty + NUM_REPEAT_WEIGHT * num_rep_penalty\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(total_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            total_train_loss += total_loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        print(f\"=> Epoch {epoch+1}/{num_epochs} completed. Train Loss: {avg_train_loss:.4f}\", flush=True)\n",
    "    return model\n",
    "\n",
    "###############################################################################\n",
    "# 8) DECODER FUNCTION\n",
    "###############################################################################\n",
    "def decode_tokens_to_string(token_seq):\n",
    "    chars = []\n",
    "    for token_id in token_seq:\n",
    "        if token_id == EOS_TOKEN_ID:\n",
    "            break\n",
    "        elif token_id in (PAD_TOKEN_ID, MASK_TOKEN_ID):\n",
    "            continue\n",
    "        chars.append(chr(token_id + 31))\n",
    "    return \"\".join(chars)\n",
    "\n",
    "###############################################################################\n",
    "# Helper: Top-k Filtering Function\n",
    "###############################################################################\n",
    "def top_k_filtering(logits, k):\n",
    "    if k <= 0:\n",
    "        return logits\n",
    "    values, _ = torch.topk(logits, k)\n",
    "    min_values = values[:, -1].unsqueeze(1)\n",
    "    return torch.where(logits < min_values, torch.full_like(logits, -float('Inf')), logits)\n",
    "\n",
    "###############################################################################\n",
    "# 9) SAMPLING FUNCTION (with stochastic reverse diffusion and top-k filtering)\n",
    "###############################################################################\n",
    "@torch.no_grad()\n",
    "def sample_passwords_diffusion(model, num_samples=100, final_temperature=1.0, base_noise_scale=0.05, top_k=10):\n",
    "    model.eval()\n",
    "    x0_sample = torch.randint(low=1, high=VOCAB_SIZE-2, size=(num_samples, MAX_SEQ_LEN), device=DEVICE)\n",
    "    x0_sample[:, -1] = EOS_TOKEN_ID\n",
    "    t_max = TIMESTEPS - 1\n",
    "    x = forward_diffusion_continuous(x0_sample, torch.tensor(t_max, device=DEVICE),\n",
    "                                     model.learned_noise_schedule, model, tau=TAU)\n",
    "    for t in reversed(range(TIMESTEPS)):\n",
    "        t_tensor = torch.full((num_samples,), t, device=DEVICE, dtype=torch.long)\n",
    "        logits = model(x, t_tensor, pre_embedded=True)\n",
    "        if t > 0:\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            token_emb_weights = model.token_emb.weight\n",
    "            predicted_x0_emb = torch.matmul(probs, token_emb_weights)\n",
    "            x = reverse_diffusion_update(predicted_x0_emb, t, model, base_noise_scale=base_noise_scale)\n",
    "        else:\n",
    "            logits = logits / final_temperature\n",
    "            flat_logits = logits.view(-1, model.vocab_size)\n",
    "            flat_logits = top_k_filtering(flat_logits, top_k)\n",
    "            probs = torch.softmax(flat_logits, dim=-1)\n",
    "            token_indices = torch.multinomial(probs, 1)\n",
    "            token_indices = token_indices.view(num_samples, MAX_SEQ_LEN)\n",
    "    samples = [decode_tokens_to_string(token_indices[i].cpu().numpy()) for i in range(num_samples)]\n",
    "    return samples\n",
    "\n",
    "###############################################################################\n",
    "# 10) MAIN FUNCTION: TRAIN MODEL & GENERATE SAMPLES\n",
    "###############################################################################\n",
    "def main():\n",
    "    dataset_file = r\"B:\\Nasko\\rockyou5MIL.txt\"\n",
    "    full_dataset = PasswordDataset(dataset_file)\n",
    "    train_loader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              drop_last=True, pin_memory=PIN_MEMORY, num_workers=NUM_WORKERS)\n",
    "    model = DiffusionTransformer(vocab_size=VOCAB_SIZE, embed_dim=EMBED_DIM,\n",
    "                                 hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS,\n",
    "                                 num_heads=NUM_HEADS, max_seq_len=MAX_SEQ_LEN).to(DEVICE)\n",
    "    model = train_model(model, train_loader, NUM_EPOCHS)\n",
    "    model_save_path = \"5MIL_Reguralization.pth\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"[DEBUG] Saved new model to {model_save_path}\", flush=True)\n",
    "    \n",
    "    samples = sample_passwords_diffusion(model, num_samples=100, final_temperature=0.7, base_noise_scale=0.05, top_k=10)\n",
    "    print(\"[DEBUG] Generated Samples:\")\n",
    "    for i, sample in enumerate(samples, 1):\n",
    "        print(f\"Sample #{i}: {sample}\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60a07cae-3d63-4975-a62e-729d3fbf3ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_13332\\4062682316.py:180: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_load_path, map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Generating 500 passwords in batches of 50...\n",
      "[DEBUG] Password generation complete. Passwords saved to 'PassDiffusion100K'.\n"
     ]
    }
   ],
   "source": [
    "#Generate passwords\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "\n",
    "###############################################################################\n",
    "# 1) FORCE CUDA USAGE OR EXIT\n",
    "###############################################################################\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"ERROR: CUDA device not available! Exiting...\", flush=True)\n",
    "    sys.exit(1)\n",
    "DEVICE = \"cuda\"\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "###############################################################################\n",
    "# 2) HYPERPARAMETERS & SETTINGS\n",
    "###############################################################################\n",
    "MAX_SEQ_LEN = 32\n",
    "VOCAB_SIZE = 300\n",
    "EOS_TOKEN_ID = VOCAB_SIZE - 1   # Stop token\n",
    "MASK_TOKEN_ID = VOCAB_SIZE - 2  # Mask token\n",
    "PAD_TOKEN_ID = 0\n",
    "\n",
    "EMBED_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 8\n",
    "NUM_HEADS = 8\n",
    "\n",
    "# Sampling hyperparameters\n",
    "FINAL_TEMPERATURE = 0.8\n",
    "BASE_NOISE_SCALE = 0.05\n",
    "TOP_K = 10\n",
    "\n",
    "# Optimized tau hyperparameter\n",
    "TAU = 0.05\n",
    "\n",
    "# Fixed cosine noise schedule (for initializing the learned noise schedule)\n",
    "def fixed_noise_schedule(timesteps, min_noise=0.02, max_noise=0.98):\n",
    "    return [min_noise + (max_noise - min_noise) * (1 - math.cos(math.pi * (i + 0.5) / timesteps)) / 2\n",
    "            for i in range(timesteps)]\n",
    "TIMESTEPS = 10\n",
    "init_schedule = fixed_noise_schedule(TIMESTEPS, min_noise=0.02, max_noise=0.98)\n",
    "init_schedule_tensor = torch.tensor(init_schedule, dtype=torch.float32)\n",
    "\n",
    "PIN_MEMORY = True\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "###############################################################################\n",
    "# 5) DIFFUSION TRANSFORMER MODEL (WITHOUT LENGTH PREDICTION HEAD)\n",
    "###############################################################################\n",
    "class DiffusionTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, num_heads, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, embed_dim)\n",
    "        self.time_emb = nn.Embedding(TIMESTEPS, embed_dim)\n",
    "\n",
    "        self.input_norm = nn.LayerNorm(embed_dim)\n",
    "        self.input_dropout = nn.Dropout(0.1)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=0.1,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "        self.learned_noise_schedule = nn.Parameter(init_schedule_tensor.clone())\n",
    "\n",
    "    def forward(self, x, t, pre_embedded=False):\n",
    "        if not pre_embedded:\n",
    "            tokens_emb = self.token_emb(x)\n",
    "        else:\n",
    "            tokens_emb = x\n",
    "        positions = torch.arange(self.max_seq_len, device=x.device).unsqueeze(0)\n",
    "        pos_emb = self.pos_emb(positions)\n",
    "        if t.dim() == 0:\n",
    "            t_embed = self.time_emb(t).unsqueeze(0).unsqueeze(1).expand(x.size(0), self.max_seq_len, -1)\n",
    "        else:\n",
    "            t_embed = self.time_emb(t).unsqueeze(1).expand(-1, self.max_seq_len, -1)\n",
    "        x_input = tokens_emb + pos_emb + t_embed\n",
    "        x_input = self.input_norm(x_input)\n",
    "        x_input = self.input_dropout(x_input)\n",
    "        encoded = self.transformer_encoder(x_input)\n",
    "        logits = self.fc_out(encoded)\n",
    "        return logits\n",
    "\n",
    "###############################################################################\n",
    "# 6) STOCHASTIC FUNCTIONS\n",
    "###############################################################################\n",
    "def forward_diffusion_continuous(x, t, noise_schedule, model, tau=TAU):\n",
    "    x0_emb = model.token_emb(x)  # shape: (B, T, D)\n",
    "    B, T, D = x0_emb.shape\n",
    "    mask_token_tensor = torch.tensor([MASK_TOKEN_ID], device=x.device)\n",
    "    mask_emb = model.token_emb(mask_token_tensor)\n",
    "    r = torch.rand(B, T, device=x.device)\n",
    "    if t.dim() == 0:\n",
    "        nf = noise_schedule[int(t.item())].item()\n",
    "        noise_fraction = torch.full((B, T), nf, device=x.device)\n",
    "    else:\n",
    "        noise_fraction = noise_schedule[t].unsqueeze(1).expand(B, T)\n",
    "    m = torch.sigmoid((r - noise_fraction) / tau)\n",
    "    m = m.unsqueeze(-1)\n",
    "    return (1 - m) * x0_emb + m * mask_emb\n",
    "\n",
    "def reverse_diffusion_update(predicted_x0_emb, t, model, base_noise_scale=BASE_NOISE_SCALE):\n",
    "    if t == 0:\n",
    "        return predicted_x0_emb\n",
    "    noise_fraction = model.learned_noise_schedule[t - 1]\n",
    "    mask_token_tensor = torch.tensor([MASK_TOKEN_ID], device=predicted_x0_emb.device)\n",
    "    mask_emb = model.token_emb(mask_token_tensor)\n",
    "    mask_emb = mask_emb.unsqueeze(0).expand_as(predicted_x0_emb)\n",
    "    noise_scale = base_noise_scale * (t / TIMESTEPS)\n",
    "    noise = noise_scale * torch.randn_like(predicted_x0_emb)\n",
    "    return (1 - noise_fraction) * predicted_x0_emb + noise_fraction * mask_emb + noise\n",
    "\n",
    "def top_k_filtering(logits, k):\n",
    "    if k <= 0:\n",
    "        return logits\n",
    "    values, _ = torch.topk(logits, k)\n",
    "    min_values = values[:, -1].unsqueeze(1)\n",
    "    return torch.where(logits < min_values, torch.full_like(logits, -float('Inf')), logits)\n",
    "\n",
    "def sample_passwords_diffusion(model, num_samples=100, final_temperature=FINAL_TEMPERATURE,\n",
    "                               base_noise_scale=BASE_NOISE_SCALE, top_k=TOP_K):\n",
    "    model.eval()\n",
    "    x0_sample = torch.randint(low=1, high=VOCAB_SIZE-2, size=(num_samples, MAX_SEQ_LEN), device=DEVICE)\n",
    "    x0_sample[:, -1] = EOS_TOKEN_ID  # force EOS\n",
    "    t_max = TIMESTEPS - 1\n",
    "    x = forward_diffusion_continuous(x0_sample, torch.tensor(t_max, device=DEVICE),\n",
    "                                     model.learned_noise_schedule, model, tau=TAU)\n",
    "    for t in reversed(range(TIMESTEPS)):\n",
    "        t_tensor = torch.full((num_samples,), t, device=DEVICE, dtype=torch.long)\n",
    "        logits = model(x, t_tensor, pre_embedded=True)\n",
    "        if t > 0:\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            token_emb_weights = model.token_emb.weight\n",
    "            predicted_x0_emb = torch.matmul(probs, token_emb_weights)\n",
    "            x = reverse_diffusion_update(predicted_x0_emb, t, model, base_noise_scale=BASE_NOISE_SCALE)\n",
    "        else:\n",
    "            logits = logits / final_temperature\n",
    "            flat_logits = logits.view(-1, model.vocab_size)\n",
    "            flat_logits = top_k_filtering(flat_logits, top_k)\n",
    "            probs = torch.softmax(flat_logits, dim=-1)\n",
    "            token_indices = torch.multinomial(probs, 1)\n",
    "            token_indices = token_indices.view(num_samples, MAX_SEQ_LEN)\n",
    "    samples = []\n",
    "    for i in range(num_samples):\n",
    "        s = []\n",
    "        for token in token_indices[i]:\n",
    "            if token.item() == EOS_TOKEN_ID:\n",
    "                break\n",
    "            if token.item() in (PAD_TOKEN_ID, MASK_TOKEN_ID):\n",
    "                continue\n",
    "            s.append(chr(token.item() + 31))\n",
    "        samples.append(\"\".join(s))\n",
    "    return samples\n",
    "\n",
    "###############################################################################\n",
    "# 7) MAIN FUNCTION: LOAD MODEL, GENERATE SAMPLES IN BATCHES, AND WRITE TO FILE\n",
    "###############################################################################\n",
    "def main():\n",
    "    # Initialize model architecture and load saved weights.\n",
    "    model = DiffusionTransformer(vocab_size=VOCAB_SIZE, embed_dim=EMBED_DIM,\n",
    "                                 hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS,\n",
    "                                 num_heads=NUM_HEADS, max_seq_len=MAX_SEQ_LEN).to(DEVICE)\n",
    "    model_load_path = \"KL_divergence.pth\"  # Adjust path as needed.\n",
    "    model.load_state_dict(torch.load(model_load_path, map_location=DEVICE))\n",
    "    model.eval()\n",
    "    \n",
    "    output_file = \"PassDiffusion100K\"\n",
    "    total_passwords = 500\n",
    "    batch_size = 50\n",
    "    num_batches = total_passwords // batch_size\n",
    "    remainder = total_passwords % batch_size\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"latin-1\") as f:\n",
    "        print(f\"[DEBUG] Generating {total_passwords} passwords in batches of {batch_size}...\")\n",
    "        for batch in range(num_batches):\n",
    "            samples = sample_passwords_diffusion(model, num_samples=batch_size, final_temperature=FINAL_TEMPERATURE,\n",
    "                                                  base_noise_scale=BASE_NOISE_SCALE, top_k=TOP_K)\n",
    "            for sample in samples:\n",
    "                f.write(sample + \"\\n\")\n",
    "            if (batch + 1) % 100 == 0:\n",
    "                print(f\"[DEBUG] Completed batch {batch + 1}/{num_batches}\")\n",
    "        \n",
    "        # If there's a remainder, generate and write them as well.\n",
    "        if remainder > 0:\n",
    "            samples = sample_passwords_diffusion(model, num_samples=remainder, final_temperature=FINAL_TEMPERATURE,\n",
    "                                                  base_noise_scale=BASE_NOISE_SCALE, top_k=TOP_K)\n",
    "            for sample in samples:\n",
    "                f.write(sample + \"\\n\")\n",
    "    \n",
    "    print(f\"[DEBUG] Password generation complete. Passwords saved to '{output_file}'.\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
